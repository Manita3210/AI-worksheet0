{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing required libraries"
      ],
      "metadata": {
        "id": "a7s4EThr1YCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris, load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n"
      ],
      "metadata": {
        "id": "-2F8m_QOV2HX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Custom Decision Tree implementation"
      ],
      "metadata": {
        "id": "24RT0lY4V4s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        probs = np.bincount(y) / len(y)\n",
        "        return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        wl = len(left) / len(parent)\n",
        "        wr = len(right) / len(parent)\n",
        "        return self._entropy(parent) - (wl * self._entropy(left) + wr * self._entropy(right))\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        if len(np.unique(y)) == 1 or (self.max_depth and depth >= self.max_depth):\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        best_gain = -1\n",
        "        best_split = None\n",
        "\n",
        "        for feature in range(X.shape[1]):\n",
        "            for threshold in np.unique(X[:, feature]):\n",
        "                left = y[X[:, feature] <= threshold]\n",
        "                right = y[X[:, feature] > threshold]\n",
        "                if len(left) == 0 or len(right) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, left, right)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_split = (feature, threshold)\n",
        "\n",
        "        if best_split is None:\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        feature, threshold = best_split\n",
        "        mask = X[:, feature] <= threshold\n",
        "\n",
        "        return {\n",
        "            'feature': feature,\n",
        "            'threshold': threshold,\n",
        "            'left': self._build_tree(X[mask], y[mask], depth+1),\n",
        "            'right': self._build_tree(X[~mask], y[~mask], depth+1)\n",
        "        }\n",
        "\n",
        "    def _predict_one(self, x, tree):\n",
        "        if 'class' in tree:\n",
        "            return tree['class']\n",
        "        if x[tree['feature']] <= tree['threshold']:\n",
        "            return self._predict_one(x, tree['left'])\n",
        "        return self._predict_one(x, tree['right'])\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict_one(x, self.tree) for x in X]"
      ],
      "metadata": {
        "id": "wBO1huPyV9KW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Iris dataset - Training and evaluation"
      ],
      "metadata": {
        "id": "kJBnNB7MWB--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "custom_tree = CustomDecisionTree(max_depth=3)\n",
        "custom_tree.fit(X_train, y_train)\n",
        "custom_preds = custom_tree.predict(X_test)\n",
        "\n",
        "sk_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sk_tree.fit(X_train, y_train)\n",
        "sk_preds = sk_tree.predict(X_test)\n",
        "\n",
        "custom_acc = accuracy_score(y_test, custom_preds)\n",
        "sk_acc = accuracy_score(y_test, sk_preds)\n",
        "\n",
        "print(\"Decision Tree Accuracy Comparison (Iris Dataset):\\n\")\n",
        "print(f\"Custom Decision Tree Accuracy      : {custom_acc:.4f}\")\n",
        "print(f\"Scikit-learn Decision Tree Accuracy: {sk_acc:.4f}\")\n",
        "\n",
        "if custom_acc > sk_acc:\n",
        "    print(\"Observation: Custom tree performs better on unseen data.\")\n",
        "elif custom_acc < sk_acc:\n",
        "    print(\"Observation: Scikit-learn tree performs better due to optimizations.\")\n",
        "else:\n",
        "    print(\"Observation: Both models perform equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-w4q4AQWKCt",
        "outputId": "4ae0d20d-d624-4ff7-c178-5e928ed879a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy Comparison (Iris Dataset):\n",
            "\n",
            "Custom Decision Tree Accuracy      : 1.0000\n",
            "Scikit-learn Decision Tree Accuracy: 1.0000\n",
            "Observation: Both models perform equally well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Ensemble Methods- classification (Wine dataset)"
      ],
      "metadata": {
        "id": "QXgMa1OjWPmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "dt_f1 = f1_score(y_test, dt.predict(X_test), average='weighted')\n",
        "rf_f1 = f1_score(y_test, rf.predict(X_test), average='weighted')\n",
        "\n",
        "print(\"F1 Score Comparison (Wine Dataset) :\\n\")\n",
        "print(f\"Decision Tree Classifier F1 Score : {dt_f1:.4f}\")\n",
        "print(f\"Random Forest Classifier F1 Score : {rf_f1:.4f}\")\n",
        "\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"Random Forest generally achieves a higher F1 score because\")\n",
        "print(\"it combines multiple decision trees and reduces overfitting.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtoUzUClWYEP",
        "outputId": "6649bd3f-7984-4672-d9f5-1afea6911154"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score Comparison (Wine Dataset) :\n",
            "\n",
            "Decision Tree Classifier F1 Score : 0.9440\n",
            "Random Forest Classifier F1 Score : 1.0000\n",
            "\n",
            "Conclusion:\n",
            "Random Forest generally achieves a higher F1 score because\n",
            "it combines multiple decision trees and reduces overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Hyperparameter Tuning - GridSearchCV"
      ],
      "metadata": {
        "id": "sYgobrFNWbk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    scoring='f1_weighted',\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"GridSearchCV Results:\\n\")\n",
        "print(\"Best Hyperparameters:\")\n",
        "for k, v in grid.best_params_.items():\n",
        "    print(f\" - {k}: {v}\")\n",
        "\n",
        "print(f\"Best Cross-Validated F1 Score: {grid.best_score_:.4f}\")\n",
        "print(\"GridSearchCV evaluates multiple combinations using cross-validation.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVdsd_gcWiic",
        "outputId": "ae05c1b2-9578-4424-f203-84f7e2bce614"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GridSearchCV Results:\n",
            "\n",
            "Best Hyperparameters:\n",
            " - max_depth: None\n",
            " - min_samples_split: 2\n",
            " - n_estimators: 100\n",
            "Best Cross-Validated F1 Score: 0.9783\n",
            "GridSearchCV evaluates multiple combinations using cross-validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Regression models"
      ],
      "metadata": {
        "id": "dVATswhgWlSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = wine.data, wine.target.astype(float)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "\n",
        "dt_reg.fit(X_train, y_train)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "dt_mse = mean_squared_error(y_test, dt_reg.predict(X_test))\n",
        "rf_mse = mean_squared_error(y_test, rf_reg.predict(X_test))\n",
        "\n",
        "print(\"Regression Performance (MSE):\\n\")\n",
        "print(f\"Decision Tree Regressor MSE : {dt_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE : {rf_mse:.4f}\")\n",
        "print(\"Lower MSE indicates better predictive performance.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHlupt_yWpje",
        "outputId": "1e2edb87-1dd0-4eef-8669-ef13daaa3264"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Performance (MSE):\n",
            "\n",
            "Decision Tree Regressor MSE : 0.1667\n",
            "Random Forest Regressor MSE : 0.0648\n",
            "Lower MSE indicates better predictive performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Hyperparameter Tuning - RandomizedSearchCV"
      ],
      "metadata": {
        "id": "JyL4cNJdWtEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    RandomForestRegressor(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=5,\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"RandomizedSearchCV Results :\\n\")\n",
        "print(\"Best Parameters Found:\")\n",
        "for k, v in random_search.best_params_.items():\n",
        "    print(f\" - {k}: {v}\")\n",
        "\n",
        "print(f\"Best Cross-Validated Score: {random_search.best_score_:.4f}\")\n",
        "print(\"RandomizedSearchCV is efficient for large search spaces.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo9qC-4bWzmA",
        "outputId": "71adae88-c745-4f50-dbe9-b316a7cde4b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomizedSearchCV Results :\n",
            "\n",
            "Best Parameters Found:\n",
            " - n_estimators: 200\n",
            " - min_samples_split: 10\n",
            " - max_depth: None\n",
            "Best Cross-Validated Score: 0.9211\n",
            "RandomizedSearchCV is efficient for large search spaces.\n"
          ]
        }
      ]
    }
  ]
}